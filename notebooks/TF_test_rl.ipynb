{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pyvirtualdisplay\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network, actor_distribution_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=())"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the environment\n",
    "env_name = \"CartPole-v0\"\n",
    "train_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "\n",
    "# prepare the agent\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "# Prepare the policies\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),train_env.action_spec())\n",
    "train_step = train_env.reset()\n",
    "random_policy.action(train_step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "21.9"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the evaluation function, and use the random_policy as the baseline performance\n",
    "compute_avg_return(eval_env,random_policy,num_eval_episodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Memory Replay, an useful technique\n",
    "\n",
    "- to remove the interrelation in episodes?[This link](https://www.zhihu.com/question/52064135)\n",
    "- other usage?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jinghuan/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jinghuan/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "# Replay Memory/Buffer Technique\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "iterator = iter(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.4.0'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-1867f18bf9eb>:64: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-1867f18bf9eb>:64: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v2.summary' has no attribute 'scalar'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-1867f18bf9eb>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     63\u001B[0m   \u001B[0;31m# Use data from the buffer and update the agent's network.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m   \u001B[0mexperience\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreplay_buffer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgather_all\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m   \u001B[0mtrain_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_agent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexperience\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m   \u001B[0mreplay_buffer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    826\u001B[0m     \u001B[0mtracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    827\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtm\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 828\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    829\u001B[0m       \u001B[0mcompiler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"xla\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_experimental_compile\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"nonXla\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    830\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    869\u001B[0m       \u001B[0;31m# This is the first call of __call__, so we have to initialize.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    870\u001B[0m       \u001B[0minitializers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 871\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_initialize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madd_initializers_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minitializers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    872\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    873\u001B[0m       \u001B[0;31m# At this point we know that the initialization is complete (or less\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_initialize\u001B[0;34m(self, args, kwds, add_initializers_to)\u001B[0m\n\u001B[1;32m    723\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_graph_deleter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mFunctionDeleter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lifted_initializer_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m     self._concrete_stateful_fn = (\n\u001B[0;32m--> 725\u001B[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m    726\u001B[0m             *args, **kwds))\n\u001B[1;32m    727\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_get_concrete_function_internal_garbage_collected\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2967\u001B[0m       \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2968\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2969\u001B[0;31m       \u001B[0mgraph_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2970\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2971\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   3359\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3360\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3361\u001B[0;31m           \u001B[0mgraph_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3362\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3363\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[1;32m   3194\u001B[0m     \u001B[0marg_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbase_arg_names\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mmissing_arg_names\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3195\u001B[0m     graph_function = ConcreteFunction(\n\u001B[0;32m-> 3196\u001B[0;31m         func_graph_module.func_graph_from_py_func(\n\u001B[0m\u001B[1;32m   3197\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3198\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001B[0m\n\u001B[1;32m    988\u001B[0m         \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moriginal_func\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_decorator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munwrap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpython_func\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    989\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 990\u001B[0;31m       \u001B[0mfunc_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mfunc_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfunc_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    991\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    992\u001B[0m       \u001B[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36mwrapped_fn\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m    632\u001B[0m             \u001B[0mxla_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mExit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    633\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 634\u001B[0;31m           \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mweak_wrapped_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    635\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tf_agents/agents/tf_agent.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, experience, weights, **kwargs)\u001B[0m\n\u001B[1;32m    516\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    517\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_enable_functions\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 518\u001B[0;31m       loss_info = self._train_fn(\n\u001B[0m\u001B[1;32m    519\u001B[0m           experience=experience, weights=weights, **kwargs)\n\u001B[1;32m    520\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tf_agents/utils/common.py\u001B[0m in \u001B[0;36mwith_check_resource_vars\u001B[0;34m(*fn_args, **fn_kwargs)\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m         \u001B[0;31m# autodep-like behavior is already expected of fn.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 185\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mfn_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfn_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    186\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mresource_variables_enabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mMISSING_RESOURCE_VARIABLES_ERROR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tf_agents/agents/reinforce/reinforce_agent.py\u001B[0m in \u001B[0;36m_train\u001B[0;34m(self, experience, weights)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    267\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGradientTape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 268\u001B[0;31m       loss_info = self.total_loss(\n\u001B[0m\u001B[1;32m    269\u001B[0m           \u001B[0mexperience\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop_gradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreturns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    270\u001B[0m           training=True)\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tf_agents/agents/reinforce/reinforce_agent.py\u001B[0m in \u001B[0;36mtotal_loss\u001B[0;34m(self, experience, returns, weights, training)\u001B[0m\n\u001B[1;32m    396\u001B[0m     \u001B[0mlosses_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'total_loss'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtotal_loss\u001B[0m  \u001B[0;31m# Total loss not in loss_info_extra.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 398\u001B[0;31m     common.summarize_scalar_dict(losses_dict,\n\u001B[0m\u001B[1;32m    399\u001B[0m                                  \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_step_counter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    400\u001B[0m                                  name_scope='Losses/')\n",
      "\u001B[0;32m~/anaconda3/envs/anormally_detection/lib/python3.8/site-packages/tf_agents/utils/common.py\u001B[0m in \u001B[0;36msummarize_scalar_dict\u001B[0;34m(name_data, step, name_scope)\u001B[0m\n\u001B[1;32m   1367\u001B[0m       \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mname_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1368\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1369\u001B[0;31m           tf.compat.v2.summary.scalar(\n\u001B[0m\u001B[1;32m   1370\u001B[0m               name=name, data=data, step=step)\n\u001B[1;32m   1371\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow.compat.v2.summary' has no attribute 'scalar'"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()\n",
    "eval_policy = tf_agent.policy\n",
    "replay_buffer_capacity = 2000 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration=2\n",
    "collect_policy = tf_agent.collect_policy\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "  episode_counter = 0\n",
    "  environment.reset()\n",
    "\n",
    "  while episode_counter < num_episodes:\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "    if traj.is_boundary():\n",
    "      episode_counter += 1\n",
    "\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "  collect_episode(\n",
    "      train_env, tf_agent.collect_policy, collect_episodes_per_iteration)\n",
    "\n",
    "  # Use data from the buffer and update the agent's network.\n",
    "  experience = replay_buffer.gather_all()\n",
    "  train_loss = tf_agent.train(experience)\n",
    "  replay_buffer.clear()\n",
    "\n",
    "  step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "Discrete(2)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = suite_gym.load(env_name)\n",
    "env.observation_spec().shape\n",
    "env.action_space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}